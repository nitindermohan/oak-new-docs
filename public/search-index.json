[{"content":"Well-thought-through product announcements will help increase feature awareness and engage users with new functionality. Just like sharing your public roadmap, it\u0026rsquo;s also a great way to let potential customers see that you\u0026rsquo;re constantly improving.\nFurther reading Read How to announce product updates and features ","date":"2023-09-07","id":0,"permalink":"/blog/example-post/","summary":"You can use blog posts for announcing product updates and features.","tags":[],"title":"Example Post"},{"content":"","date":"2023-09-07","id":1,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"","date":"2023-09-07","id":2,"permalink":"/docs/getting-started/","summary":"","tags":[],"title":"Getting Started"},{"content":"\rOakestra lets you deploy your workload on devices of any size. From a small RasperryPi to a cloud instance far away on GCP or AWS. The tree structure enables you to create multiple clusters of resources.\nThe Root Orchestrator manages different clusters of resources. The root only sees aggregated cluster resources. The Cluster orchestrator manages your worker nodes. This component collects the real-time resources and schedules your workloads to the perfect matching device. A Worker is any device where a component called NodeEngine is installed. Each node can support multiple execution environments such as Containers (containerd runtime), MicroVM (containerd runtime), and Unikernels (mirageOS). Did you know?\nSince the stable Accordion release, Oakestra supports both containers and unikernel virtualization targets.\nCreate your first Oakestra cluster Let\u0026rsquo;s start simple with a single node deployment, where all the components are deployed on the same device.\nRequirements\nLinux (Workers only) Docker + Docker compose (Orchestrators only) Cluster Orchestrator and Root Orchestrator machines must be mutually reachable. 1-DOC (1 Device, One Cluster) In this example, we will use a single device to deploy all the components. This is not recommended for production environments, but it is pretty cool for home environments and development.\n0) First, let\u0026rsquo;s export the required environment variables\n## Choose a unique name for your cluster export CLUSTER_NAME=My_Awesome_Cluster ## Come up with a name for the current location export CLUSTER_LOCATION=My_Awesome_Apartment ## Tell the NetManager where to find the system manager export SYSTEM_MANAGER_URL=\u0026lt;IP of device\u0026gt;\rNote\nYou can obtain the public IPv4 address of your device with\ncurl -4 https://ifconfig.co\r1) Clone the repository and move into it using:\ngit clone https://github.com/oakestra/oakestra.git \u0026amp;\u0026amp; cd oakestra\r2) Run a local 1-DOC cluster\nsudo -E docker-compose -f run-a-cluster/1-DOC.yaml up\r3) In another terminal download, untar and install the node engine package\nwget -c https://github.com/oakestra/oakestra/releases/download/v0.4.202/NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; mv NodeEngine NodeEngine_$(dpkg --print-architecture) \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture)\r4) Download, unzip and install the network manager; this enables an overlay network across your services\nwget -c https://github.com/oakestra/oakestra-net/releases/download/v0.4.202/NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture)\r4.1) Edit /etc/netmanager/netcfg.json as follows:\n{ \u0026#34;NodePublicAddress\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THIS DEVICE\u0026gt;\u0026#34;, \u0026#34;NodePublicPort\u0026#34;: \u0026#34;\u0026lt;PORT REACHABLE FROM OUTSIDE, use 50103 as default\u0026gt;\u0026#34;, \u0026#34;ClusterUrl\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ClusterMqttPort\u0026#34;: \u0026#34;10003\u0026#34; }\r4.2) (Optional) Start the NetManager on port 6000\nsudo NetManager -p 6000 \u0026amp;\r5) Start the NodeEngine. Please only use the -n 6000 parameter if you started the network component in step 4. This parameter, in fact, is used to specify the internal port of the network component, if any.\nsudo NodeEngine -n 6000 -p 10100\r( you can use NodeEngine -h for further details )\nM-DOC (M Devices, One Cluster) Now, let\u0026rsquo;s separate the Oakestra components over multiple devices and create a more distributed cluster.\nThe M-DOC deployment enables you to deploy One cluster with multiple worker nodes. The main difference between this deployment and 1-DOC is that the worker nodes might be external here, and there can be multiple of them.\nThe deployment of this kind of cluster is similar to 1-DOC. We first need to start the root and cluster orchestrator. Afterward, we can attach the worker nodes.\n1) On the node you wish to use as a cluster and root orchestrator, execute steps 1-DOC.1 and 1-DOC.2\n2) Now, we need to prepare all the worker nodes. On each worker node, execute the following:\n2.1) Download and unpack both the NodeEngine\nwget -c https://github.com/oakestra/oakestra/releases/download/v0.4.202/NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NodeEngine_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; mv NodeEngine NodeEngine_$(dpkg --print-architecture) \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture)\rand the NetManager\nwget -c https://github.com/oakestra/oakestra-net/releases/download/v0.4.202/NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; tar -xzf NetManager_$(dpkg --print-architecture).tar.gz \u0026amp;\u0026amp; chmod +x install.sh \u0026amp;\u0026amp; ./install.sh $(dpkg --print-architecture)\r2.2) Edit /etc/netmanager/netcfg.json accordingly:\n{ \u0026#34;NodePublicAddress\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THIS DEVICE\u0026gt;\u0026#34;, \u0026#34;NodePublicPort\u0026#34;: \u0026#34;\u0026lt;PORT REACHABLE FROM OUTSIDE, internal port is always 50103\u0026gt;\u0026#34;, \u0026#34;ClusterUrl\u0026#34;: \u0026#34;\u0026lt;IP ADDRESS OF THE CLUSTER ORCHESTRATOR\u0026gt;\u0026#34;, \u0026#34;ClusterMqttPort\u0026#34;: \u0026#34;10003\u0026#34; }\r2.3) Run the NetManager and the NodeEngine components:\nsudo NetManager -p 6000 \u0026amp; sudo NodeEngine -n 6000 -p 10100 -a \u0026lt;IP ADDRESS OF THE CLUSTER ORCHESTRATOR\u0026gt;\rMDNC (M Devices, N Clusters) This represents the most versatile deployment. You can split your resources into multiple clusters within different locations and with different resources. In this deployment, we need to deploy the root and the cluster orchestrator on different nodes. Each independent cluster orchestrator represents a cluster of resources. The worker nodes attached to each cluster are aggregated and seen as a unique big resource from the point of view of the Root. This deployment isolates the resources from the root perspective and delegates the responsibility to the cluster orchestrator. 1) In this first step, we need to deploy the root orchestrator component on a node. To do this, you need to clone the repository on the desired node, move to the root orchestrator folder, and execute the startup command.\ngit clone https://github.com/oakestra/oakestra.git \u0026amp;\u0026amp; cd oakestra sudo -E docker-compose -f root_orchestrator/docker-compose-\u0026lt;arch\u0026gt;.yml up\r( please replace \u0026lt; arch \u0026gt; with your device architecture: arm or amd64 )\n2) For each node that needs to host a cluster orchestrator, you need to:\n2.1) Export the ENV variables needed to connect to the cluster orchestrator:\nexport SYSTEM_MANAGER_URL=\u0026lt;IP ADDRESS OF THE NODE HOSTING THE ROOT ORCHESTRATOR\u0026gt; export CLUSTER_NAME=\u0026lt;choose a name for your cluster\u0026gt; export CLUSTER_LOCATION=\u0026lt;choose a name for the cluster\u0026#39;s location\u0026gt;\r2.2) Clone the repo and run the cluster orchestrator:\ngit clone https://github.com/oakestra/oakestra.git \u0026amp;\u0026amp; cd oakestra sudo -E docker-compose -f cluster_orchestrator/docker-compose-\u0026lt;arch\u0026gt;.yml up\r( please replace \u0026lt; arch \u0026gt; with your device architecture: arm or amd64 )\n3) Start and configure each worker as described in M-DOC.2\nHybrid Deployments You should have got the gist now, but if you want, you can build the infrastructure by composing the components like LEGO blocks. Do you want to give your Cluster Orchestrator computational capabilities for the deployment? Deploy an instance of NodeEngine + Netmanager components, and you\u0026rsquo;re done. You don\u0026rsquo;t want to use a separate node for the Root Orchestrator? Simply deploy it all together with a cluster orchestrator.\nWith Oakestra you can build your infrastructure as you like, and you can always scale it up or down as you need!\n","date":"0001-01-01","id":3,"permalink":"/docs/getting-started/deploy-your-first-oakestra-cluster/","summary":"\u003cp\u003e\r\n\r\n\u003cimg\r\n  src=\"/docs/getting-started/highLevelArch_hu13515324087746045157.webp\"\r\n  width=\"1362\"\r\n  height=\"684\"\r\n  decoding=\"async\"\r\n  fetchpriority=\"auto\"\r\n  loading=\"lazy\"\r\n  alt=\"High level architecture picture\"id=\"h-rh-i-0\"\r\n/\u003e\u003c/p\u003e\n\u003cp\u003eOakestra lets you deploy your workload on devices of any size. From a small RasperryPi to a cloud instance far away on GCP or AWS. The tree structure enables you to create multiple clusters of resources.\u003c/p\u003e","tags":[],"title":"Deploy your first Oakestra Cluster"},{"content":"","date":"2023-09-07","id":4,"permalink":"/docs/getting-started/deploy-app/","summary":"","tags":[],"title":"Deploy your first App"},{"content":"\rRequirements\nYou have a running Oakestra deployment. You have at least one Worker Node registered You have installed and properly configured the NetManager. You can access the APIs at \u0026lt;root-orch-ip\u0026gt;:10000/api/docs Let\u0026rsquo;s try deploying an Nginx server and a client. Then we\u0026rsquo;ll enter inside the client container and try to curl Nginx webserver.\nAll we need to do to deploy an application is to create a deployment descriptor and submit it to the platform using the APIs.\nDeployment Descriptor In order to deploy a container a deployment descriptor must be passed to the deployment command. The deployment descriptor contains all the information that Oakestra needs in order to achieve a complete deploy in the system.\nOakestra uses the following deployment descriptor format.\ndeploy_curl_application.yaml\r{ \u0026#34;sla_version\u0026#34; : \u0026#34;v2.0\u0026#34;, \u0026#34;customerID\u0026#34; : \u0026#34;Admin\u0026#34;, \u0026#34;applications\u0026#34; : [ { \u0026#34;applicationID\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;application_name\u0026#34; : \u0026#34;clientserver\u0026#34;, \u0026#34;application_namespace\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;application_desc\u0026#34; : \u0026#34;Simple demo with curl client and Nginx server\u0026#34;, \u0026#34;microservices\u0026#34; : [ { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;curl\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;tail -f /dev/null\u0026#34;], \u0026#34;memory\u0026#34;: 100, \u0026#34;vcpus\u0026#34;: 1, \u0026#34;vgpus\u0026#34;: 0, \u0026#34;vtpus\u0026#34;: 0, \u0026#34;bandwidth_in\u0026#34;: 0, \u0026#34;bandwidth_out\u0026#34;: 0, \u0026#34;storage\u0026#34;: 0, \u0026#34;code\u0026#34;: \u0026#34;docker.io/curlimages/curl:7.82.0\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9080\u0026#34;, \u0026#34;added_files\u0026#34;: [] }, { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;cmd\u0026#34;: [], \u0026#34;memory\u0026#34;: 100, \u0026#34;vcpus\u0026#34;: 1, \u0026#34;vgpus\u0026#34;: 0, \u0026#34;vtpus\u0026#34;: 0, \u0026#34;bandwidth_in\u0026#34;: 0, \u0026#34;bandwidth_out\u0026#34;: 0, \u0026#34;storage\u0026#34;: 0, \u0026#34;code\u0026#34;: \u0026#34;docker.io/library/nginx:latest\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;6080:80/tcp\u0026#34;, \u0026#34;addresses\u0026#34;: { \u0026#34;rr_ip\u0026#34;: \u0026#34;10.30.30.30\u0026#34; }, \u0026#34;added_files\u0026#34;: [] } ] } ] }\rSave this description as deploy_curl_application.yaml and upload it to the system using the APIs.\nThis deployment descriptor example generates one application named clientserver with the test namespace and two microservices:\nnginx server with test namespace, namely clientserver.test.nginx.test curl client with test namespace, namely clientserver.test.curl.test Learn more about the SLA specifications\rLogin to the APIs After running a cluster you can use the debug OpenAPI page at \u0026lt;root_orch_ip\u0026gt;:10000/api/docs to interact with the apis and use the infrastructure.\nAuthenticate using the following procedure:\nLocate the login method and use the try-out button Use the default Admin credentials to login { \u0026#34;username\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;\u0026#34; }\rCopy the result login token Go to the top of the page and authenticate with this token Register an application and the services After you authenticate with the login function, you can deploy your first application.\nUpload the deployment descriptor to the system. You can try the one above. The response contains the Application id and the id for all the application\u0026rsquo;s services. Now the application and the services are registered to the platform. It\u0026rsquo;s time to deploy the service instances!\nYou can always remove or create a new service for the application using the /api/services endpoints\nDeploy an instance of a registered service Trigger a deployment of a service\u0026rsquo;s instance using POST /api/service/{serviceid}/instance Each call to this endpoint generates a new instance of the service\nMonitor the service status With GET /api/applications/\u0026lt;userid\u0026gt; (or simply /api/applications/ if you\u0026rsquo;re admin) you can check the list of the deployed application. With GET /api/services/\u0026lt;appid\u0026gt; you can check the services attached to an application With GET /api/service/\u0026lt;serviceid\u0026gt; you can check the status for all the instances of \u0026lt;serviceid\u0026gt; Undeploy the service Use DELETE /api/service/\u0026lt;serviceid\u0026gt; to delete all the instances of a service Use DELETE /api/service/\u0026lt;serviceid\u0026gt;/instance/\u0026lt;instance number\u0026gt; to delete a specific instance of a service Use DELETE /api/application/\u0026lt;appid\u0026gt; to delete all together an application with all the services and instances Check if the service (un)deployment succeeded Familiarize yourself with the API and discover for each one of the service the status and the public address.\nIf both services are ACTIVE, it is time to test the communication.\nIf either of the services are not ACTIVE, there might be a configuration issue or a bug. You can check the logs of the NetManager and NodeEngine components with docker logs system_manager -f --tail=1000 on the root orchestrator, with docker logs cluster_manager -f --tail=1000 on the cluster orchestrator. If unable to resolve, please open an issue on GitHub.\nTry to reach the nginx server you just deployed.\nhttp://\u0026lt;deployment_machine_ip\u0026gt;:6080\rIf you see the Nginx landing page, you just deployed your very first application with Oakestra! Hurray! ðŸŽ‰\n","date":"2023-09-07","id":5,"permalink":"/docs/getting-started/deploy-app/with-the-api/","summary":"\u003cdiv class=\"callout callout-caution d-flex flex-row mt-4 mb-4 pt-4 pe-4 pb-2 ps-3\"\u003e\r\n  \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"24\"\n  height=\"24\"\n  viewBox=\"0 0 24 24\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-width=\"2\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n \n class=\"outline/alert-triangle svg-inline callout-icon me-2 mb-3\" id=\"svg-alert-triangle\" role=\"img\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\"/\u003e\n  \u003cpath d=\"M12 9v4\" /\u003e\n  \u003cpath d=\"M10.363 3.591l-8.106 13.534a1.914 1.914 0 0 0 1.636 2.871h16.214a1.914 1.914 0 0 0 1.636 -2.87l-8.106 -13.536a1.914 1.914 0 0 0 -3.274 0z\" /\u003e\n  \u003cpath d=\"M12 16h.01\" /\u003e\n\u003c/svg\u003e\r\n  \u003cdiv class=\"callout-content\"\u003e\r\n    \u003cdiv class=\"callout-title\"\u003e\r\n        \u003cp\u003eRequirements\u003c/p\u003e","tags":[],"title":"With the API"},{"content":"The Dashboard Now that you have familiarized yourself with the API, you can try out the dashboard. The dashboard is the front-end component to cluster management.\nIt allows the user to:\nView the applications currently running on the cluster Create and modify individual services Check the status of running services Configure service-level agreements (SLAs) Deployment Requirements\nYou have a running Root Orchestrator. You can access the APIs at \u0026lt;IP_OF_CLUSTER_ORCHESTRATOR\u0026gt;:10000 If you deployed your cluster with one of the provided docker-compose files, this has already been done for you and you can simply head over to \u0026lt;http://IP_OF_CLUSTER_ORCHESTRATOR\u0026gt;. Otherwise follow the instructions below.\nManual deployment: 0) Clone the repository\ngit clone https://github.com/oakestra/dashboard.git \u0026amp;\u0026amp; cd dashboard\r1) Create a file that contains the environment variables\necho \u0026#34;API_ADDRESS=\u0026lt;IP_OF_CLUSTER_ORCHESTRATOR\u0026gt;:10000\u0026#34; \u0026gt; .env\r2) Run the dashboard\nsudo docker-compose up\rRunning the Oakestra Framework To be able to log into the dashboard, the System Manager and MongoDB must be started. How this can be done is described in the getting started section.\nIf these components were not started or improperly configured, the login screen can be reached, but you cannot log in to the dashboard.\nAccessing the dashboard Now that the dashboard is up and running, let\u0026rsquo;s log in and explore its functionality. Upon launching the system for the first time, an administrative user is automatically created. This user can create and manage other users and organizations within the system, more on User Management later.\nAdmin Credentials\nUsername: Admin\nPassword: Admin\nChange the Password\nAfter setting up the cluster manager immediately change the password of the admin user!\nOrganization Login To log in to an organization check the Organization login box and enter the organization name. If the box is not checked or the organization name is left empty, then you will logged in to the default root organization.\nOrganizations\rMore on organizations\nHere you can see the login to the sampleOrga:\nApplications, Services, Namespaces In Oakestra there are applications, services and namespaces. One Application can encompass multiple services and one user can create multiple applications on one system. Namespaces allow you to create applications and services by the same name in different namespaces, e.g. production and development.\nCreating an Application First you will have to create an application. Choose a concise name, the namespace and optionally a description.\nCreating a Service In the previous section we discussed registering deployment descriptors via the API. This is great for automated deployments, but the SLAs were not designed with human readability in mind. While the dashboard still allows you to upload SLAs as a JSON file, it also provides you with an interactive form.\nOnce you have created an application you can create services. Once again you will have to choose a concise name, a namespace and optionally a description. However this is far from it; system requirements, environmental variables, connection details and much more can be specified here.\nYou will have to choose a virtualization method (Container or Unikernel) and tell Oakestra where it can find your code. Hit save and your service is ready for deployment!\nService Details Once a service has been created and deployed, you can check on it\u0026rsquo;s status and other details. Choose a service from the Service List and from the drop-down menu, choose an instance and click on View Instance Details.\nSomething Missing?\nIf you have any new feature ideas or if you find any bugs please open an issue in the GitHub repository.\n","date":"2023-09-07","id":6,"permalink":"/docs/getting-started/deploy-app/with-the-dashboard/","summary":"\u003ch2 id=\"the-dashboard\"\u003eThe Dashboard\u003c/h2\u003e\n\u003cp\u003eNow that you have familiarized yourself with the API, you can try out the dashboard. The dashboard is the front-end component to cluster management.\u003c/p\u003e","tags":[],"title":"With the Dashboard"},{"content":"","date":"2023-09-07","id":7,"permalink":"/docs/getting-started/deploy-app/with-the-cli/","summary":"","tags":[],"title":"With the CLI"},{"content":"","date":"2023-09-07","id":8,"permalink":"/docs/concepts/","summary":"","tags":[],"title":"Concepts"},{"content":"Oakestra Architecture As shown in our getting started guide, Oakestra requires 4 key building blocks to operate:\nThe Root Orchestrator The Cluster Orchestrator The Node Engine, and The Net Manager Root Orchestrator The root orchestrator is the centralized control plane that coordinates the participating clusters.\nThe above image describes the components of the root orchestrator. Each component is deployed as a separate container and docker-compose is used to integrate and run them.\nThe System Manager is the interface by which users access the the system as an application deployment platform. It exposes 2 sets of APIs: To receive deployment commands from users To handle child Oakestra Clusters The Scheduler calculates a suitable cluster for a given application Mongo is the interface used to access the database. The root manager stores aggregated information on it\u0026rsquo;s child clusters. Oakestra differentiates between Static metadata such as the IP-address, port number, name and location of each cluster Dynamic data such as worker nodes per cluster, total CPU cores and memory, total disk space, GPU capabilities, etc\u0026hellip; Failure and scalability The key drawback of a centralized control plane is that is creates a single point of failure. Oakestra mitigates this by ensuring that the clusters are able to satisfy the SLAs for deployed applications, the only affected functionalities are the deployment of new services and intra-cluster migrations.\nCluster Orchestrator The cluster orchestrator is a twin of the root, with the following distinctions:\nThe scope of the cluster orchestrator the is worker nodes A key duty performed is aggregation. A cluster orchestrator aggregates the worker node resources and obscures the cluster composition to the root MQTT is used for intra-cluster communication Scheduling At each level, schedulers receive job placement tasks and return a placement decision. At the root level a cluster is chosen for a given service. At the cluster level a worker node is chosen.\nA job placement task is comprised of a service with it\u0026rsquo;s instances and resource requirements. Currently placement follows the best-fit algorithm.\nWorker Node A worker node is a machine running the NodeEngine and the NetManager. The former enables the deployment of applications according to the runtimes installed. The latter provides networking components to enable inter-application communication.\nThe NodeEngine is a single binary implemented using Go and is composed of the following modules:\nMQTT: The interface between the worker and the cluster. Deployment commands, node status updates and job updates use this component. Models: Models that describe the nodes and jobs Node: Describes the resources that are transmitted to the cluster. These are decomposed into static resources, which are only transmitted at startup, and dynamic resources, which are periodically updated, e.g. CPU/memory Service: Describes the services that are managed by this worker node, as well as the real-time service usage statistics Jobs: Background jobs that monitor the the status of the worker node and the deployed applications Runtimes: The supported system runtimes. Currently containers and Unikernels are supported ","date":"0001-01-01","id":9,"permalink":"/docs/concepts/high-level-architecture/","summary":"\u003ch1 id=\"oakestra-architecture\"\u003eOakestra Architecture\u003c/h1\u003e\n\u003cp\u003eAs shown in our \u003ca href=\"../../getting-started/deploy-your-first-oakestra-cluster/\"\u003egetting started\u003c/a\u003e guide, Oakestra requires 4 key building blocks to operate:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ca href=\"#root-orchestrator\"\u003eRoot Orchestrator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe \u003ca href=\"#cluster-orchestrator\"\u003eCluster Orchestrator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe \u003ca href=\"#node-engine\"\u003eNode Engine\u003c/a\u003e, and\u003c/li\u003e\n\u003cli\u003eThe \u003ca href=\"#net-manager\"\u003eNet Manager\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"root-orchestrator\"\u003eRoot Orchestrator\u003c/h1\u003e\n\u003cp\u003eThe root orchestrator is the centralized control plane that coordinates the participating clusters.\u003c/p\u003e","tags":[],"title":"High Level Architecture"},{"content":"","date":"0001-01-01","id":10,"permalink":"/docs/concepts/orchestration/","summary":"","tags":[],"title":"Orchestration"},{"content":"","date":"0001-01-01","id":11,"permalink":"/docs/concepts/scheduling/","summary":"","tags":[],"title":"Scheduling"},{"content":"","date":"0001-01-01","id":12,"permalink":"/docs/concepts/networking/","summary":"","tags":[],"title":"Networking"},{"content":"","date":"2023-09-07","id":13,"permalink":"/docs/concepts/features/","summary":"","tags":[],"title":"Features"},{"content":"","date":"0001-01-01","id":14,"permalink":"/docs/concepts/features/performance/","summary":"","tags":[],"title":"Performance"},{"content":"","date":"0001-01-01","id":15,"permalink":"/docs/concepts/features/heterogeneity/","summary":"","tags":[],"title":"Heterogeneity"},{"content":"","date":"2023-09-07","id":16,"permalink":"/docs/manuals/","summary":"","tags":[],"title":"Manuals"},{"content":"","date":"0001-01-01","id":17,"permalink":"/docs/manuals/unikernel-operations/","summary":"","tags":[],"title":"Unikernel Operations"},{"content":"","date":"0001-01-01","id":18,"permalink":"/docs/manuals/kubernetes-integration/","summary":"","tags":[],"title":"Kubernetes Integration"},{"content":"","date":"2023-09-07","id":19,"permalink":"/docs/manuals/networking-internals/","summary":"","tags":[],"title":"Networking Internals"},{"content":"","date":"0001-01-01","id":20,"permalink":"/docs/manuals/networking-internals/semantic-addressing/","summary":"","tags":[],"title":"Semantic Addressing"},{"content":"","date":"0001-01-01","id":21,"permalink":"/docs/manuals/networking-internals/ipv4-addressing/","summary":"","tags":[],"title":"IPv4 Addressing"},{"content":"","date":"0001-01-01","id":22,"permalink":"/docs/manuals/networking-internals/ipv6-addressing/","summary":"","tags":[],"title":"IPv6 Addressing"},{"content":"","date":"0001-01-01","id":23,"permalink":"/docs/manuals/networking-internals/gateway-operations/","summary":"","tags":[],"title":"Gateway Operations"},{"content":"","date":"2023-09-07","id":24,"permalink":"/docs/manuals/dashboard-features/","summary":"","tags":[],"title":"Dashboard Features"},{"content":"Organizations We briefly mentioned organizations previously, now let\u0026rsquo;s take a closer look at them. Organizations facilitate collaboration amongst team members by providing:\nA collaborative Space: All applications within an organization are visible to all team members Resource Utilization: Resources are shared within an organization Access Control: Easily grant a new team member access to your applications and resources by adding them to your organization The Root Organization A user can be a member of multiple organizations, but they are all a member of the root organization. This is the default organization that is created when a root orchestrator is started, it is also the organization that you sign in to when you do not select organization login. Unlike other organizations, applications and resources are not shared and are only available to the user that created them.\nUser Management Different team members have different tasks that require different permissions, so it makes sense to assign them roles based on which permissions they require to do their work.\nRoles Admin: Can add new users to an organization and manage their roles. This is the role of the default user Organization Admin: Can add new users to an organization and manage their roles Infrastructure Provider: Can add resources to the organization Application Provider: The default role of a user, can manage and deploy applications on organization resources ","date":"0001-01-01","id":25,"permalink":"/docs/manuals/dashboard-features/organizations/","summary":"\u003ch2 id=\"organizations\"\u003eOrganizations\u003c/h2\u003e\n\u003cp\u003eWe briefly mentioned organizations previously, now let\u0026rsquo;s take a closer look at them.\nOrganizations facilitate collaboration amongst team members by providing:\u003c/p\u003e","tags":[],"title":"Organizations"},{"content":"","date":"0001-01-01","id":26,"permalink":"/docs/manuals/dashboard-features/email-configuration/","summary":"","tags":[],"title":"Email Configuration"},{"content":"","date":"2023-09-07","id":27,"permalink":"/docs/manuals/cli-features/","summary":"","tags":[],"title":"CLI Features"},{"content":"","date":"0001-01-01","id":28,"permalink":"/docs/manuals/cli-features/features/","summary":"","tags":[],"title":"Features"},{"content":"","date":"2023-09-07","id":29,"permalink":"/docs/manuals/debugging/","summary":"","tags":[],"title":"Debugging"},{"content":"","date":"0001-01-01","id":30,"permalink":"/docs/manuals/debugging/networking/","summary":"","tags":[],"title":"Networking"},{"content":"","date":"0001-01-01","id":31,"permalink":"/docs/manuals/debugging/control-plane/","summary":"","tags":[],"title":"Control Plane"},{"content":"","date":"0001-01-01","id":32,"permalink":"/docs/manuals/debugging/unikernels/","summary":"","tags":[],"title":"Unikernels"},{"content":"","date":"2023-09-07","id":33,"permalink":"/docs/contribution-guide/","summary":"","tags":[],"title":"Contributing Guide"},{"content":"","date":"0001-01-01","id":34,"permalink":"/docs/contribution-guide/code-of-conduct/","summary":"","tags":[],"title":"Code of Conduct"},{"content":"","date":"0001-01-01","id":35,"permalink":"/docs/contribution-guide/commit-hooks/","summary":"","tags":[],"title":"Commit Hooks"},{"content":"","date":"0001-01-01","id":36,"permalink":"/docs/contribution-guide/language-guide/","summary":"","tags":[],"title":"Language Guide"},{"content":"","date":"2023-09-07","id":37,"permalink":"/docs/guides/","summary":"","tags":[],"title":"Guides"},{"content":"Guides lead a user through a specific task they want to accomplish, often with a sequence of steps. Writing a good guide requires thinking about what your users are trying to do.\nFurther reading Read about how-to guides in the DiÃ¡taxis framework ","date":"2023-09-07","id":38,"permalink":"/docs/guides/example-guide/","summary":"\u003cp\u003eGuides lead a user through a specific task they want to accomplish, often with a sequence of steps. Writing a good guide requires thinking about what your users are trying to do.\u003c/p\u003e","tags":[],"title":"Example Guide"},{"content":"","date":"2023-09-07","id":39,"permalink":"/docs/reference/","summary":"","tags":[],"title":"Reference"},{"content":"Application Deployment SLA The SLA deplyment descriptor is a JSON file that describes the deployment of an application in the Oakestra platform.\nAn example SLA of application X with two microservices X1 and X3 can be as follows.\n{ \u0026#34;sla_version\u0026#34; : \u0026#34;v2.0\u0026#34;, \u0026#34;customerID\u0026#34; : \u0026#34;Admin\u0026#34;, \u0026#34;applications\u0026#34; : [ { \u0026#34;applicationID\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;application_name\u0026#34; : \u0026#34;X\u0026#34;, \u0026#34;application_namespace\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;application_desc\u0026#34; : \u0026#34;X application\u0026#34;, \u0026#34;microservices\u0026#34; : [ { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;X1\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;vcpu\u0026#34;: 1, \u0026#34;storage\u0026#34;: 100, \u0026#34;code\u0026#34;: \u0026#34;docker.io/X/X1\u0026#34;, \u0026#34;addresses\u0026#34;: { \u0026#34;rr_ip\u0026#34;: \u0026#34;10.30.0.1\u0026#34; }, }, { \u0026#34;microserviceID\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;microservice_name\u0026#34;: \u0026#34;X3\u0026#34;, \u0026#34;microservice_namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;vcpu\u0026#34;: 2, \u0026#34;storage\u0026#34;: 200, \u0026#34;virtualization\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;code\u0026#34;: \u0026#34;docker.io/X/X3\u0026#34;, \u0026#34;addresses\u0026#34;: { \u0026#34;rr_ip\u0026#34;: \u0026#34;10.30.1.30\u0026#34; }, } ] } ] }\rFormat The file is composed of the following fields:\nsla_version: the current version customerID: id of the user, default is Admin application list, in a single deployment descriptor it is possible to define multiple applications, each containing: Fully qualified app name: A fully qualified name in Oakestra is composed of\napplication_name: unique name representing the application (max 10 char, no symbols) application_namespace: namespace of the app, used to reference different deployment of the same application. Examples of namespace name can be default or production or test (max 10 char, no symbols) applicationID: leave it empty for new deployments, this is needed only to edit an existing deployment. application_desc: Short description of the application\nmicroservice list is a list of the microservices composing the application. For each microservice the user can specify:\nmicroserviceID: leave it empty for new deployments, this is needed only to edit an existing deployment.\nFully qualified service name: Similar to application name, it is composed of\nmicroservice_name: name of the service (max 10 char, no symbols) microservice_namespace: namespace of the service, used to reference different deployment of the same service. Examples of namespace name can be default or production or test (max 10 char, no symbols) virtualization: Starting with \u0026#x1fa97; Accordion v0.4.301, Oakestra supports both container and unikernel virtualization\ncmd: list of the commands to be executed inside the container at startup\nvcpu: minimum cpu vcores needed to run the container\nvgpu: minimum gpu vcores needed to run the container\nmemory: minimum memory amount needed to run the container\nstorage: minimum storage size required (currently the scheduler does not take this value into account)\nbandwidth_in/out: minimum required bandwith on the worker node. (currently the scheduler does not take this value into account)\nport: port mapping for the container in the syntax hostport_1:containerport_1[/protocol];hostport_2:containerport_2[/protocol] (default protocol is tcp)\naddresses: allows to specify a custom ip address to be used to balance the traffic across all the service instances.\nrr\\_ip: [optional filed] This field allows you to setup a custom round-robin network address to reference all the instances belonging to this service. This address is going to be permanently bounded to the service. The address MUST be in the form 10.30.x.y and must not collide with any other Instance Address or Service IP in the system, otherwise an error will be returned. If you don\u0026rsquo;t specify a RR_ip and you don\u0026rsquo;t set this field, a new address will be generated by the system. constraints: array of constraints regarding the service.\ntype: constraint type direct: Send a deployment to a specific cluster and a specific list of eligible nodes. You can specify \u0026quot;node\u0026quot;:\u0026quot;node1;node2;...;noden\u0026quot; a list of node\u0026rsquo;s hostnames. These are the only eligible worker nodes. \u0026quot;cluster\u0026quot;:\u0026quot;cluster_name\u0026quot; The name of the cluster where this service must be scheduled. E.g.: \u0026#34;constraints\u0026#34;:[\r{\r\u0026#34;type\u0026#34;:\u0026#34;direct\u0026#34;,\r\u0026#34;node\u0026#34;:\u0026#34;xavier1\u0026#34;,\r\u0026#34;cluster\u0026#34;:\u0026#34;gpu\u0026#34;\r}\r]\r","date":"2023-09-07","id":40,"permalink":"/docs/reference/application-sla-description/","summary":"\u003ch2 id=\"application-deployment-sla\"\u003eApplication Deployment SLA\u003c/h2\u003e\n\u003cp\u003eThe SLA deplyment descriptor is a JSON file that describes the deployment of an application in the Oakestra platform.\u003c/p\u003e","tags":[],"title":"Application SLA Description"},{"content":"","date":"2023-09-07","id":41,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2023-09-07","id":42,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":43,"permalink":"/","summary":"","tags":[],"title":"Oakestra"},{"content":"","date":"0001-01-01","id":44,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":45,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":46,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]